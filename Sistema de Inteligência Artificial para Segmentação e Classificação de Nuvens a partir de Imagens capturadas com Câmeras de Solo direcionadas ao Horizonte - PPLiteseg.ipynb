{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRuij_kd_qFo",
        "outputId": "d3fb543d-9140-434a-c0aa-beea5eb15c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 18 12:45:08 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    43W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQV5AAVIKcxh",
        "outputId": "47f9914a-6f7a-4987-8cd7-433f60c4bc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 18 12:45:09 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    43W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "!/opt/bin/nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaPWPqMhbcQo"
      },
      "source": [
        "# Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYkgGEeRbbWH",
        "outputId": "a35078de-8a23-4ada-b99a-748845eb4484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the dataset"
      ],
      "metadata": {
        "id": "rMlC9jP7gDzg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KVzS0tVv6w-"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/Shareddrives/Nuvens/datasets/clouds1500_no_tree.zip\" /content/\n",
        "!mkdir /content/clouds1500_no_tree\n",
        "!unzip \"clouds1500_no_tree.zip\" -d /content/clouds1500_no_tree"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing specific version of PaddlePaddle-GPU Lib that works with the PaddleSeg Lib"
      ],
      "metadata": {
        "id": "KDST-NRDgLxi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fRSGeDEbXLd"
      },
      "outputs": [],
      "source": [
        "# Change the working directory to '/content/'\n",
        "%cd '/content/'\n",
        "\n",
        "# Copy the HrNetBruno.yaml configuration file from Google Drive to the current working directory\n",
        "!cp 'path_to_save/train.yaml' .\n",
        "\n",
        "# Install the specified version of paddlepaddle-gpu from the provided URL\n",
        "!pip install paddlepaddle-gpu==2.4.2.post117 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html\n",
        "\n",
        "# Run a check to ensure the paddle installation is successful and working\n",
        "paddle.utils.run_check()\n",
        "\n",
        "# Print the paddle version\n",
        "print(paddle.__version__)\n",
        "\n",
        "import paddle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU-o_OQYbueA"
      },
      "source": [
        "# Installing PaddleSeg Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL0daOpRboQe"
      },
      "outputs": [],
      "source": [
        "# Clone the PaddleSeg repository\n",
        "!git clone -b release/2.7 https://github.com/PaddlePaddle/PaddleSeg.git\n",
        "\n",
        "# Change the working directory to 'PaddleSeg'\n",
        "%cd PaddleSeg\n",
        "\n",
        "# Install the required dependencies for PaddleSeg\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Run a shell script to check the installation of PaddleSeg\n",
        "!sh tests/run_check_install.sh\n",
        "\n",
        "# Install the PaddleSeg package\n",
        "!python setup.py install\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYHxevkhb9Yx"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMOkR3oRbtf8"
      },
      "outputs": [],
      "source": [
        "# Change the working directory back to '/content/'\n",
        "%cd /content/\n",
        "\n",
        "# Define experiment name and save directory\n",
        "exp_name = 'MixVisionTransformer_B0'\n",
        "sav_dir = f'path_to_save/{exp_name}'\n",
        "\n",
        "# Change the working directory to '/content/'\n",
        "%cd /content/\n",
        "\n",
        "# Create the save directory and copy the configuration file to it\n",
        "!mkdir -p $sav_dir\n",
        "!cp /content/train.yaml $sav_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iICg1akHrh_v"
      },
      "source": [
        "# Validation\n",
        "\n",
        "The code snippet performs model validation using the PaddleSeg library:\n",
        "\n",
        "1. `model_params = f'{sav_dir}/best_model/model.pdparams'`: This line creates a string with the file path of the best model's parameters from the training process. The path is constructed using the `sav_dir` variable, which contains the directory where the model and training results were saved.\n",
        "\n",
        "2. The `!python /content/PaddleSeg/val.py \\` command runs the PaddleSeg validation script (`val.py`). The following arguments are passed to the script:\n",
        "\n",
        "   - `--config /content/HrNetBruno.yaml`: This specifies the configuration file (`HrNetBruno.yaml`) to use during the validation process. The configuration file contains information about the model architecture, dataset, and other settings.\n",
        "\n",
        "   - `--model_path $model_params`: This provides the path to the best model's parameters, which were saved during the training process. The script will use these parameters to evaluate the model's performance on the validation dataset.\n",
        "\n",
        "In summary, this code snippet evaluates the trained model on the validation dataset using the specified configuration file and model parameters. The PaddleSeg validation script calculates various performance metrics, such as accuracy and IoU (Intersection over Union), to assess the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lorkxULnrhWT"
      },
      "outputs": [],
      "source": [
        "model_params = f'{sav_dir}/best_model/model.pdparams'\n",
        "\n",
        "!python /content/PaddleSeg/tools/val.py \\\n",
        "       --config /content/train.yaml \\\n",
        "       --model_path $model_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5Hh-iJMsz_2"
      },
      "source": [
        "# Test\n",
        "\n",
        "The code snippet performs model validation using the PaddleSeg library with an updated configuration file for validation:\n",
        "\n",
        "1. `model_params = f'{sav_dir}/best_model/model.pdparams'`: This line creates a string with the file path of the best model's parameters from the training process. The path is constructed using the `sav_dir` variable, which contains the directory where the model and training results were saved.\n",
        "\n",
        "2. The `!python /content/PaddleSeg/val.py \\` command runs the PaddleSeg validation script (`val.py`). The following arguments are passed to the script:\n",
        "\n",
        "   - `--config '/content/drive/Shareddrives/Nuvens/datasets/Albedo(merged classes)_001 - 997 images/Experimentos/HrNetBruno_for_validation.yaml'`: This specifies an alternative configuration file (`HrNetBruno_for_validation.yaml`) located in a different path to use during the validation process. The configuration file contains information about the model architecture, dataset, and other settings specifically for validation.\n",
        "\n",
        "   - `--model_path $model_params`: This provides the path to the best model's parameters, which were saved during the training process. The script will use these parameters to evaluate the model's performance on the validation dataset.\n",
        "\n",
        "In summary, this code snippet evaluates the trained model on the validation dataset using the specified alternative configuration file and model parameters. The PaddleSeg validation script calculates various performance metrics, such as accuracy and IoU (Intersection over Union), to assess the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBQGe2VSs1ZO"
      },
      "outputs": [],
      "source": [
        "model_params = f'{sav_dir}/best_model/model.pdparams'\n",
        "\n",
        "!python /content/PaddleSeg/tools/val.py \\\n",
        "       --config 'path_to_save/val.yaml' \\\n",
        "       --model_path $model_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN4D9mgQgCGO"
      },
      "source": [
        "# Inference\n",
        "\n",
        "The code snippet performs model prediction on a folder of images using the trained model and the PaddleSeg library:\n",
        "\n",
        "1. `model_params = f'{sav_dir}/best_model/model.pdparams'`: This line creates a string with the file path of the best model's parameters from the training process. The path is constructed using the `sav_dir` variable, which contains the directory where the model and training results were saved.\n",
        "\n",
        "2. The image folder, destination folder, and the custom color palette are defined:\n",
        "   - `image_folder`: Folder containing the input images to perform prediction on.\n",
        "   - `dest_folder`: Folder where the prediction results will be saved.\n",
        "   - `color_pallet`: A custom color palette for visualizing the predicted segmentation masks, defined as a sequence of RGB values separated by spaces.\n",
        "\n",
        "3. The `!python /content/PaddleSeg/predict.py \\` command runs the PaddleSeg prediction script (`predict.py`). The following arguments are passed to the script:\n",
        "\n",
        "   - `--config /content/HrNetBruno.yaml`: This specifies the configuration file (`HrNetBruno.yaml`) to use during the prediction process. The configuration file contains information about the model architecture, dataset, and other settings.\n",
        "\n",
        "   - `--model_path $model_params`: This provides the path to the best model's parameters, which were saved during the training process. The script will use these parameters to perform prediction on the input images.\n",
        "\n",
        "   - `--image_path $image_folder`: This specifies the folder containing the input images for prediction.\n",
        "\n",
        "   - `--save_dir  $dest_folder`: This defines the folder where the prediction results (segmentation masks) will be saved.\n",
        "\n",
        "   - `--custom_color $color_pallet`: This provides the custom color palette for visualizing the predicted segmentation masks.\n",
        "\n",
        "In summary, this code snippet performs semantic segmentation prediction on a folder of images using the trained model, specified configuration file, and model parameters. The prediction results (segmentation masks) are saved in the specified destination folder with the provided custom color palette for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKksx3Fztgvq"
      },
      "outputs": [],
      "source": [
        "# Predict Folder with trained model\n",
        "# Files created in the training Experiment\n",
        "model_params = f'{sav_dir}/best_model/model.pdparams'\n",
        "\n",
        "# Folder to predict\n",
        "image_folder = \"'/content/clouds1500_no_tree/test'\"\n",
        "\n",
        "# Folder to save the predictions\n",
        "dest_folder = f'{sav_dir}/predictions'\n",
        "\n",
        "# Custom color pallet, the format is a sequential RGB value for each class, and all values are separated by a space.\n",
        "# In the example bellow, 0 0 0 is the value for the class zero, 7 25 163 is the value for the class one and so and on.\n",
        "color_pallet = '31 119 180 43 160 43 214 39 39 148 103 189 140 85 76 '\n",
        "\n",
        "!python /content/PaddleSeg/tools/predict.py \\\n",
        "       --config /content/train.yaml \\\n",
        "       --model_path $model_params \\\n",
        "       --image_path $image_folder \\\n",
        "       --save_dir  $dest_folder \\\n",
        "       --custom_color $color_pallet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4xOAjIMdsZp"
      },
      "source": [
        "# Yaml Explanation (no need to execute this cell)\n",
        "Consider the following YAML file.\n",
        "\n",
        "The YAML (Yet Another Markup Language) file contains the configuration settings for training and validating a semantic segmentation model using the PaddleSeg library. It defines the dataset, model architecture, loss function, learning rate scheduler, optimizer, and other settings. The file is used in the previous code snippets to configure the training, validation, and prediction processes.\n",
        "\n",
        "Here's an explanation of each section in the YAML file:\n",
        "\n",
        "1. `batch_size`: The number of samples to process in each batch during training.\n",
        "2. `iters`: The total number of iterations for the training process.\n",
        "\n",
        "3. `train_dataset`: Configuration settings for the training dataset.\n",
        "   - `type`: Specifies the type of dataset (in this case, `Dataset`).\n",
        "   - `separator`: The character used to separate fields in the dataset files.\n",
        "   - `dataset_root`: The root directory of the dataset.\n",
        "   - `train_path`: The path to the file containing the training image and label paths.\n",
        "   - `num_classes`: The number of classes in the dataset.\n",
        "   - `transforms`: A list of data augmentation techniques to apply during training (in this case, `Resize`).\n",
        "   - `mode`: The mode of the dataset (in this case, `train`).\n",
        "\n",
        "4. `val_dataset`: Configuration settings for the validation dataset, similar to the `train_dataset` settings.\n",
        "\n",
        "5. `model`: Configuration settings for the model architecture.\n",
        "   - `type`: Specifies the type of model (in this case, `OCRNet`).\n",
        "   - `backbone`: Contains settings for the model's backbone architecture (in this case, `HRNet_W18`).\n",
        "   - `num_classes`: The number of classes in the dataset.\n",
        "   - `backbone_indices`: Indices of the backbone output layers to be used.\n",
        "\n",
        "6. `loss`: Configuration settings for the loss function.\n",
        "   - `types`: A list of loss functions to be used (in this case, two instances of `CrossEntropyLoss`).\n",
        "   - `coef`: A list of coefficients to apply to each loss function.\n",
        "\n",
        "7. `lr_scheduler`: Configuration settings for the learning rate scheduler.\n",
        "   - `type`: Specifies the type of learning rate scheduler (in this case, `PolynomialDecay`).\n",
        "   - `learning_rate`: The initial learning rate.\n",
        "   - `power`: The exponent used in the polynomial decay.\n",
        "\n",
        "8. `optimizer`: Configuration settings for the optimizer.\n",
        "   - `type`: Specifies the type of optimizer (in this case, `AdamW`).\n",
        "   - `beta1` and `beta2`: Parameters for the AdamW optimizer.\n",
        "   - `weight_decay`: The weight decay applied to the optimizer.\n",
        "\n",
        "In the previous code snippets, the YAML file is used to configure the training, validation, and prediction processes by providing the settings for the dataset, model architecture, loss function, learning rate scheduler, and optimizer. The `--config` argument in the `train.py`, `val.py`, and `predict.py` scripts specifies the path to the YAML file, which is then used by the PaddleSeg library to set up the required configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcveKY14d2zl"
      },
      "outputs": [],
      "source": [
        "# batch_size: 2\n",
        "# iters: 80000\n",
        "# train_dataset:\n",
        "#   type: Dataset\n",
        "#   separator: ;\n",
        "#   dataset_root: /content/drive/Shareddrives/Nuvens/datasets/Albedo(merged classes)_001 - 997 images\n",
        "#   train_path: /content/drive/Shareddrives/Nuvens/datasets/Albedo(merged classes)_001 - 997 images/train-paddle.txt\n",
        "#   num_classes: 6\n",
        "#   transforms:\n",
        "#     - type: Resize\n",
        "#       target_size: [1280, 1280]\n",
        "#   mode: train\n",
        "\n",
        "# val_dataset:\n",
        "#   type: Dataset\n",
        "#   separator: ;\n",
        "#   dataset_root: /content/drive/Shareddrives/Nuvens/datasets/Albedo(merged classes)_001 - 997 images\n",
        "#   val_path: /content/drive/Shareddrives/Nuvens/datasets/Albedo(merged classes)_001 - 997 images/val-paddle.txt\n",
        "#   num_classes: 6\n",
        "#   transforms:\n",
        "#     - type: Resize\n",
        "#       target_size: [1280, 1280]\n",
        "#   mode: val\n",
        "\n",
        "# model:\n",
        "#   type: OCRNet\n",
        "#   backbone:\n",
        "#     type: HRNet_W18\n",
        "#     pretrained: https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz\n",
        "#   num_classes: 6\n",
        "#   backbone_indices: [0]\n",
        "\n",
        "# loss:\n",
        "#   types:\n",
        "#     - type: CrossEntropyLoss\n",
        "#     - type: CrossEntropyLoss\n",
        "#   coef: [1, 0.4]\n",
        "\n",
        "# lr_scheduler:\n",
        "#   type: PolynomialDecay\n",
        "#   learning_rate: 0.0001\n",
        "#   power: 0.9\n",
        "\n",
        "# optimizer:\n",
        "#   type: AdamW\n",
        "#   beta1: 0.9\n",
        "#   beta2: 0.999\n",
        "#   weight_decay: 0.01"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}