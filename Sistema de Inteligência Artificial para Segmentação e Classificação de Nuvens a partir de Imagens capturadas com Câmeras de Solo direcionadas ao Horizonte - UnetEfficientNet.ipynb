{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAA6F70Wo1wt"
      },
      "source": [
        "# Docs\n",
        "*   https://albumentations.ai/docs/examples/pytorch_semantic_segmentation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsfR9hcco_Cu"
      },
      "source": [
        "# Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKnHZ2GLevuC",
        "outputId": "fe1742de-f3a9-482a-a0b4-71d9260aa3fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue May  9 10:14:34 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 531.79                 Driver Version: 531.79       CUDA Version: 12.1     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4090       WDDM | 00000000:01:00.0  On |                  Off |\n",
            "|  0%   44C    P8               35W / 450W|   1693MiB / 24564MiB |     40%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      1772    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "|    0   N/A  N/A      6592    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
            "|    0   N/A  N/A      8344    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
            "|    0   N/A  N/A     11780    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A     13416    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A     14276    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
            "|    0   N/A  N/A     14736    C+G   ...\\Local\\slack\\app-4.32.122\\slack.exe    N/A      |\n",
            "|    0   N/A  N/A     16188    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rN05J1O9033",
        "outputId": "24f79ea7-3f09-4cf2-f709-4899d6ad54e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "!/opt/bin/nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N46pEGY_FX7h"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmln_bd1ok4h",
        "outputId": "7057ab03-a3d9-451c-f08a-4142aa4e6a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘clouds1500_no_tree’: File exists\r\n"
          ]
        }
      ],
      "source": [
        "# Get the dataset\n",
        "!mkdir clouds1500_no_tree\n",
        "!tar -xf \"/home/Datasets/clouds1500_no_tree.tar\" -C clouds1500_no_tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu-HTyxIVD-Y"
      },
      "source": [
        "## Install required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-g5yOh_U9jy",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install albumentations==0.4.6\n",
        "!pip install efficientunet-pytorch\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mOw8debU3l6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch\n",
        "from efficientunet import *\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "# from google.colab import drive\n",
        "\n",
        "\n",
        "# Set GPU device to use\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoCghgbyVG6L"
      },
      "source": [
        "# Define directory paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMPYfxz9VGfY",
        "outputId": "1ac210a6-73ea-49f4-93dd-086b45ae75f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1227 28 223\n"
          ]
        }
      ],
      "source": [
        "root_directory = os.path.join(\"clouds1500_no_tree/\")\n",
        "images_directory = os.path.join(root_directory, \"train\")\n",
        "masks_directory = os.path.join(root_directory, \"train_mask\")\n",
        "\n",
        "test_images_directory = os.path.join(root_directory, \"test\")\n",
        "test_masks_directory = os.path.join(root_directory, \"test_mask\")\n",
        "\n",
        "val_images_directory = os.path.join(root_directory, \"validation\")\n",
        "val_masks_directory = os.path.join(root_directory, \"validation_mask\")\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "train_images_filenames = list(sorted(os.listdir(images_directory)))\n",
        "test_images_filenames = list(sorted(os.listdir(test_images_directory)))\n",
        "val_images_filenames = list(sorted(os.listdir(val_images_directory)))\n",
        "\n",
        "\n",
        "print(len(train_images_filenames), len(val_images_filenames), len(test_images_filenames))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1JIwtcpWIxb"
      },
      "source": [
        "# Pre-process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZM-UjVPpMhB"
      },
      "source": [
        "This function takes a mask image as input and converts it into a format that can be used as ground truth for training. It maps each unique color in the mask image (representing different classes) to a corresponding integer label. The function processes the mask by iterating through its pixels, converting the colors to their corresponding labels using a dictionary, and reshaping the mask back to its original dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gAWioVJX1N-"
      },
      "outputs": [],
      "source": [
        "def preprocess_mask(mask):\n",
        "    # Define colors in the mask for each class\n",
        "    background = str([0, 0, 0])\n",
        "    color_estratocumuliformes = str([1,1,1])\n",
        "    color_estratiformes = str([2,2,2])\n",
        "    color_cirriformes = str([3,3,3])\n",
        "    color_cumuliformes = str([4,4,4])\n",
        "\n",
        "    # Map colors to integer labels\n",
        "    labels = {background:0, color_estratocumuliformes:1, color_estratiformes:2, color_cirriformes:3, color_cumuliformes:4}\n",
        "\n",
        "    width = mask.shape[1]\n",
        "    height = mask.shape[0]\n",
        "\n",
        "    # Convert mask colors to corresponding integer labels\n",
        "    values = [str(list(mask[i,j])) for i in range(height) for j in range(width)]\n",
        "    mask = [labels[value] for value in values]\n",
        "\n",
        "    # Reshape mask to the original dimensions\n",
        "    mask = np.asarray(mask).reshape(height, width)\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG2dB41oXd9r"
      },
      "source": [
        "This class inherits from the PyTorch Dataset class and is designed to handle the images and masks of the dataset. The constructor takes the filenames, directories, and an optional transform as input. The class has two main methods: __len__ returns the number of images in the dataset, and __getitem__ reads the image and mask at the specified index, preprocesses the mask using the preprocess_mask function, and applies any provided transformations to the images and masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvVvrDSDaSz3"
      },
      "outputs": [],
      "source": [
        "class OxfordPetDataset(Dataset):\n",
        "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n",
        "        self.images_filenames = images_filenames\n",
        "        self.images_directory = images_directory\n",
        "        self.masks_directory = masks_directory\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Read image and mask based on the provided index\n",
        "        image_filename = self.images_filenames[idx]\n",
        "        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(\n",
        "            os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,\n",
        "        )\n",
        "\n",
        "        # Preprocess the mask\n",
        "        mask = preprocess_mask(mask)\n",
        "\n",
        "        # Apply provided transformations to the image and mask\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, mask=mask)\n",
        "            image = transformed[\"image\"]\n",
        "            mask = transformed[\"mask\"]\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bNT_1HCY9cn"
      },
      "source": [
        "`train_transform` and `val_transform` are defined using the Albumentations library to perform image augmentations and preprocessing. The transformations include resizing the images and converting them to PyTorch tensors. For the training set, more augmentations can be uncommented for data augmentation, and normalization can be added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC6Z9WwvaqMb"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(1280, 1280),\n",
        "#         A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
        "#         A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
        "#         A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
        "#         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "train_dataset = OxfordPetDataset(train_images_filenames, images_directory, masks_directory, transform=train_transform,)\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(1280, 1280),\n",
        "        # A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "        #             std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "     ]\n",
        ")\n",
        "val_dataset = OxfordPetDataset(val_images_filenames, val_images_directory, val_masks_directory, transform=val_transform,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEYFvgSTZOnT"
      },
      "source": [
        "The `MetricMonitor` class provides a convenient way to monitor and display the performance of different metrics during the training or evaluation of a machine learning model. The reset method can be called to reset the metrics before each epoch or evaluation, while the update method can be used to update the metrics with new values after each batch or evaluation step. Finally, the `__str__` method can be called to display the current state of the metrics in a readable format.\n",
        "\n",
        "* `__init__(self, float_precision=3)`: This is the class constructor method that initializes the object with a default float_precision of 3. The float_precision parameter determines the number of decimal points to display in the output of the __str__ method.\n",
        "\n",
        "* `reset(self)`: This method resets all the metric values stored in the object. It does this by setting all the values of val, count, and avg to zero using a defaultdict.\n",
        "\n",
        "* `update(self, metric_name, val)`: This method updates the stored metric values with new values. It takes two parameters: metric_name, a string that represents the name of the metric to update, and val, a float value that represents the value of the metric. This method increments the count of the metric by 1 and adds the new value to the running total. It then calculates the average of the metric using the running total and count.\n",
        "\n",
        "* `__str__(self)`: This method returns a formatted string of the metric values stored in the object. It returns a string with the average values of all metrics in the object. The string contains the metric name followed by its average value rounded off to the specified float_precision decimal points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4_BTLkc6sr-"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class MetricMonitor:\n",
        "    def __init__(self, float_precision=3):\n",
        "        # Constructor method that initializes the object with a default float_precision of 3\n",
        "        self.float_precision = float_precision\n",
        "        # Calls the reset method to set the initial state of the metrics\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Resets all metric values to zero using a defaultdict with a lambda function\n",
        "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
        "\n",
        "    def update(self, metric_name, val):\n",
        "        # Updates the stored metric values with new values\n",
        "        metric = self.metrics[metric_name]\n",
        "\n",
        "        # Increment the count of the metric by 1 and add the new value to the running total\n",
        "        metric[\"val\"] += val\n",
        "        metric[\"count\"] += 1\n",
        "\n",
        "        # Calculate the average of the metric using the running total and count\n",
        "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
        "\n",
        "    def __str__(self):\n",
        "        # Returns a formatted string of the metric values stored in the object\n",
        "        return \" | \".join(\n",
        "            [\n",
        "                # Format the metric name and its average value using the float_precision parameter\n",
        "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
        "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
        "                )\n",
        "                # Iterate over all the metrics stored in the object and create a string for each one\n",
        "                for (metric_name, metric) in self.metrics.items()\n",
        "            ]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEdHwjrZbFXd"
      },
      "source": [
        "The `train` function provides a convenient way to train a PyTorch neural network model on a given dataset using a specified loss function, optimizer, and hyperparameters. It also tracks the training loss and displays the progress of the training process using a MetricMonitor object and a tqdm progress bar.\n",
        "\n",
        "The function takes several parameters:\n",
        "\n",
        "* `train_loader`: A PyTorch DataLoader object that provides the training dataset in batches\n",
        "model: A PyTorch neural network model to train\n",
        "* `criterion`: A loss function used to calculate the error between the model's predictions and the target labels\n",
        "* `optimizer`: A PyTorch optimizer that updates the model's parameters during training\n",
        "* `epoch`: An integer that represents the current epoch of training\n",
        "* `params`: A dictionary that contains various hyperparameters for the training process, including the device (CPU or GPU) to use for processing the data.\n",
        "\n",
        "The function first creates a MetricMonitor object to keep track of the training loss during training. It then sets the model to training mode and wraps the train_loader with a tqdm progress bar to display the progress of the training process.\n",
        "\n",
        "The function then loops over the batches in the train_loader and performs the following steps for each batch:\n",
        "\n",
        "* Moves the input images and target labels to the device (CPU or GPU) for faster processing\n",
        "* Computes the model's predictions for the input images using the model object\n",
        "* Calculates the loss between the model's predictions and the target labels using the criterion object\n",
        "* Updates the MetricMonitor object with the current loss value using the update method\n",
        "* Zeros out the gradients from the previous batch using the `optimizer.zero_grad()` method\n",
        "* Computes the gradients of the loss with respect to the model's parameters using the `loss.backward()` method\n",
        "* Updates the model's parameters based on the computed gradients using the `optimizer.step()` method\n",
        "* Updates the tqdm progress bar with the current epoch and the training metrics using the `stream.set_description()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o90wUmXK6wB9"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch, params):\n",
        "    metric_monitor = MetricMonitor()\n",
        "    model.train()\n",
        "    stream = tqdm(train_loader)\n",
        "    for i, (images, target) in enumerate(stream, start=1):\n",
        "        images = images.to(params[\"device\"], non_blocking=True).float()\n",
        "        target = target.to(params[\"device\"], non_blocking=True).type(torch.LongTensor)\n",
        "        output = model(images.cuda()).squeeze(1)\n",
        "        loss = criterion(output.cuda(), target.cuda())\n",
        "        metric_monitor.update(\"Loss\", loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        stream.set_description(\n",
        "            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "230bDjGGcUtd"
      },
      "source": [
        "The `validate` function provides a convenient way to validate a PyTorch neural network model on a given dataset using a specified loss function, and hyperparameters. It also tracks the validation loss and displays the progress of the validation process using a `MetricMonitor` object and a `tqdm` progress bar.\n",
        "\n",
        "The function takes several parameters:\n",
        "\n",
        "* `val_loader`: A PyTorch DataLoader object that provides the validation dataset in batches\n",
        "model: A PyTorch neural network model to validate\n",
        "* `criterion`: A loss function used to calculate the error between the model's predictions and the target labels\n",
        "* `epoch`: An integer that represents the current epoch of validation\n",
        "* `params`: A dictionary that contains various hyperparameters for the validation process, including the device (CPU or GPU) to use for processing the data.\n",
        "\n",
        "The function first creates a `MetricMonitor` object to keep track of the validation loss during validation. It then sets the model to evaluation mode (i.e., turns off dropout and batch normalization) and wraps the `val_loader` with a `tqdm` progress bar to display the progress of the validation process.\n",
        "\n",
        "The function then loops over the batches in the `val_loader` and performs the following steps for each batch:\n",
        "* Moves the input images and target labels to the device (CPU or GPU) for faster processing\n",
        "* Computes the model's predictions for the input images using the `model` object\n",
        "* Calculates the loss between the model's predictions and the target labels using the `criterion` object\n",
        "* Updates the `MetricMonitor` object with the current loss value using the update method\n",
        "* Updates the `tqdm` progress bar with the current epoch and the validation metrics using the `stream.set_description()` method.\n",
        "\n",
        "Note that the `torch.no_grad()` context manager is used to turn off gradient tracking during validation to save memory and speed up the computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4cQ960A60-H"
      },
      "outputs": [],
      "source": [
        "def validate(val_loader, model, criterion, epoch, params):\n",
        "    metric_monitor = MetricMonitor()\n",
        "    model.eval()\n",
        "    stream = tqdm(val_loader)\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(stream, start=1):\n",
        "            images = images.to(params[\"device\"], non_blocking=True).float()\n",
        "            target = target.to(params[\"device\"], non_blocking=True).type(torch.LongTensor)\n",
        "            output = model(images.cuda()).squeeze(1)\n",
        "            loss = criterion(output.cuda(), target.cuda())\n",
        "            metric_monitor.update(\"Loss\", loss.item())\n",
        "            stream.set_description(\n",
        "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFzYslnm-yIP"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8kukErxdpgN"
      },
      "source": [
        "## Training Params\n",
        "Defining PyTorch DataLoader objects, train_loader and val_loader, that provide the training and validation datasets in batches during the training process.\n",
        "\n",
        "* `device`: A string that represents the device (CPU or GPU) to use for processing the data.\n",
        "* `lr`: A float that represents the learning rate for the optimizer during training.\n",
        "* batch_size: An integer that represents the number of samples to process in each batch during training.\n",
        "* `num_workers`: An integer that represents the number of CPU workers to use for loading the data in parallel during training.\n",
        "* `epochs`: An integer that represents the number of epochs to train the model.\n",
        "\n",
        "Each DataLoader object takes several arguments:\n",
        "\n",
        "* The first argument is a PyTorch `Dataset` object that provides the dataset to load.\n",
        "* `batch_size`: An integer that represents the number of samples to process in each batch during training or validation.\n",
        "* `shuffle`: A boolean that indicates whether to shuffle the data between epochs (for the `train_loader` only).\n",
        "* `num_workers`: An integer that represents the number of CPU workers to use for loading the data in parallel.\n",
        "* `pin_memory`: A boolean that indicates whether to copy the data to pinned memory for faster transfer to the GPU (for the `train_loader` and `val_loader` objects).\n",
        "\n",
        "\n",
        "Overall, the `DataLoader` objects provide a convenient way to load the training and validation data in batches during the training process, with various hyperparameters specified in the params dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BDFQzEJ9_ky"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"device\": \"cuda\",\n",
        "    \"lr\": 0.0001,\n",
        "    \"batch_size\": 2,\n",
        "    \"num_workers\": 0,\n",
        "    \"epochs\": 1,\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=params[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=params[\"num_workers\"],\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=params[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=params[\"num_workers\"],\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN3KYfpremBJ"
      },
      "source": [
        "## Training the model\n",
        "Here we train a PyTorch neural network model called `get_efficientunet_b0` on a given dataset using the specified hyperparameters.\n",
        "\n",
        "We first creates the `get_efficientunet_b0` model and loads its weights from a pre-trained checkpoint using `load_state_dict()`. It also creates a CrossEntropyLoss loss function and an Adam optimizer with the specified learning rate and model parameters.\n",
        "\n",
        "We then train the model using a `for` loop that iterates over the specified number of epochs (from 0 to `params[\"epochs\"] + 1`). For each epoch, the code calls the train function to train the model on the training dataset and updates the model parameters using the optimizer. It also calls the `validate` function to validate the model on the validation dataset and prints the validation metrics.\n",
        "\n",
        "### **Important Notes**\n",
        "Note that we save the model checkpoint after each epoch using `torch.save()` with the model's state dictionary and the current epoch number in the file name. This allows for resuming training later by changing the range of the `for` loop to continue training from the last saved checkpoint.\n",
        "\n",
        "#### Example\n",
        "Suppose you have trained the model for `10 epochs` and want to continue training for an additional `5 epochs`. You can change the range of the for loop to start from epoch 11 and train for 5 additional epochs as follows:\n",
        "\n",
        "```python\n",
        "for epoch in range(11, 16):\n",
        "    train(train_loader, model, criterion, optimizer, epoch, params)\n",
        "    validate(val_loader, model, criterion, epoch, params)\n",
        "    torch.save(model.state_dict(), \"dir/\"+str(epoch)+\".pth\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N-dBRaGy6xoT"
      },
      "outputs": [],
      "source": [
        "# Create the EfficientUNet-B0 model with specified parameters and load its weights from a saved checkpoint\n",
        "model = get_efficientunet_b0(out_channels=6, concat_input=True, pretrained=True).to(\"cuda\")\n",
        "# model.load_state_dict(torch.load('/content/drive/Shareddrives/Nuvens/resultados_pytorch/Bruno/unet_efficientnet/clouds_1500/com_arvore/results/8.pth'))\n",
        "\n",
        "# Define the loss function and optimizer with specified hyperparameters\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
        "\n",
        "# Train the model for the specified number of epochs and save the checkpoints\n",
        "for epoch in range(0, params[\"epochs\"] + 1):\n",
        "    # Train the model on the training dataset and update the model parameters using the optimizer\n",
        "    train(train_loader, model, criterion, optimizer, epoch, params)\n",
        "\n",
        "    # Validate the model on the validation dataset and print the validation metrics\n",
        "    validate(val_loader, model, criterion, epoch, params)\n",
        "\n",
        "    # Save the model checkpoint after each epoch with the current state dictionary and epoch number\n",
        "    torch.save(model.state_dict(), \"path_to_save//\"+str(epoch)+\".pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q15eh-tweo0O"
      },
      "source": [
        "Load a pre-trained `EfficientUNet-B0` model from a saved checkpoint and sets it to evaluation mode.\n",
        "\n",
        "The code first creates the `EfficientUNet-B0` model with specified parameters using the `get_efficientunet_b0` function. The `out_channels` parameter specifies the number of output channels in the final layer, `concat_input` specifies whether to concatenate the input with the final feature map, and `pretrained` specifies whether to load the pre-trained weights. The model is then moved to the GPU using the to() method.\n",
        "\n",
        "Then, the code loads the weights of the model from a saved checkpoint using the `load_state_dict()` function. The path of the checkpoint file is specified in the argument of the `load_state_dict()` function.\n",
        "\n",
        "Finally, the code sets the model to evaluation mode using the `eval()` method. This is necessary to disable certain layers of the model (such as dropout or batch normalization) that behave differently during training and testing, and to ensure that the model outputs deterministic predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqgeuO2O_mqx"
      },
      "outputs": [],
      "source": [
        "# Load the EfficientUNet-B0 model with specified parameters and load its weights from a saved checkpoint\n",
        "model = get_efficientunet_b0(out_channels=5, concat_input=True, pretrained=True).to(\"cpu\")\n",
        "model.load_state_dict(torch.load('path_to_save/12.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtjDoWlV7FTh"
      },
      "source": [
        "# Inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ej9XWe4iFbJ"
      },
      "source": [
        "The class inherits from the Dataset class in PyTorch and overrides the `__init__()`, `__len__()`, and `__getitem__()` methods. The `__init__()` method initializes the dataset with a list of image filenames, the directory containing the images, and an optional transform to be applied to the images. The `__len__()` method returns the number of images in the dataset, and the `__getitem__()` method loads an image from disk, applies the transformation (if any), and returns the transformed image and its original size.\n",
        "\n",
        "Specifically, the code reads an image from disk using the cv2.imread() function from the OpenCV library and converts it from the BGR color format to the RGB format using `cv2.cvtColor()`. The original size of the image is also computed and returned along with the transformed image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mym36zps7Ex-"
      },
      "outputs": [],
      "source": [
        "# Define a PyTorch dataset class for inference on the Oxford-IIIT Pet dataset\n",
        "class OxfordPetInferenceDataset(Dataset):\n",
        "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n",
        "        self.images_filenames = images_filenames\n",
        "        self.images_directory = images_directory\n",
        "        self.masks_directory = masks_directory\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Read image and mask based on the provided index\n",
        "        image_filename = self.images_filenames[idx]\n",
        "        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(\n",
        "            os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,\n",
        "        )\n",
        "\n",
        "        # Preprocess the mask\n",
        "        mask = preprocess_mask(mask)\n",
        "\n",
        "        # Apply provided transformations to the image and mask\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, mask=mask)\n",
        "            image = transformed[\"image\"]\n",
        "            mask = transformed[\"mask\"]\n",
        "\n",
        "        return image, mask, image_filename\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsG83hRkiddu"
      },
      "source": [
        "Similar to the previous class, the `OxfordPetInferenceMaskDataset` class inherits from the `Dataset` class and overrides the `__init__()`, `__len__()`, and `__getitem__()` methods. The class is initialized with a list of image filenames, the directory containing the images, the directory containing the masks, and an optional transform to be applied to the masks.\n",
        "\n",
        "The `__getitem__()` method first loads the mask associated with the image using the OpenCV library, by replacing the \".jpg\" extension with \".png\" to get the mask filename. The mask is then preprocessed using a `preprocess_mask()` function (not shown in the code snippet). If a transform is provided, it is applied to the mask and the transformed mask is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyX7umkmv75_"
      },
      "outputs": [],
      "source": [
        "# Define a PyTorch dataset class for loading and transforming ground truth masks of the Oxford-IIIT Pet dataset\n",
        "class OxfordPetInferenceMaskDataset(Dataset):\n",
        "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n",
        "        self.images_filenames = images_filenames\n",
        "        self.images_directory = images_directory\n",
        "        self.masks_directory = masks_directory\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the mask associated with the image and preprocess it\n",
        "        image_filename = self.images_filenames[idx]\n",
        "        mask = cv2.imread(os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED)\n",
        "        mask = preprocess_mask(mask)\n",
        "\n",
        "        # Apply the transformation (if any) and return the transformed mask\n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=mask)\n",
        "            mask = transformed[\"image\"]\n",
        "        return mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-EPy7ccApIA"
      },
      "source": [
        "## Calculate metrics for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nSvwnTOg9d9",
        "outputId": "bbb4642b-1b02-4594-9e0d-597378c913b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean IoU: 0.4744\n",
            "IoU of each class: tensor([0.7071, 0.9408, 0.6039, 0.1992, 0.2531, 0.1424])\n",
            "class_precision tensor([0.7744, 0.9676, 0.6893, 0.5087, 0.5176, 0.4928])\n",
            "Mean Precision: 0.6584\n",
            "class_recall tensor([0.8906, 0.9714, 0.8298, 0.2466, 0.3313, 0.1668])\n",
            "Mean Recall: 0.5728\n",
            "class_f1_score tensor([0.8284, 0.9695, 0.7531, 0.3322, 0.4040, 0.2493])\n",
            "class_f1_score.mean() tensor(0.5894)\n",
            "Mean Accuracy: 0.7461181879043579\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# define the transformation to be applied to the test images\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(1280, 1280),\n",
        "        # A.Resize(2592, 1944),\n",
        "        # A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "        #             std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "     ]\n",
        ")\n",
        "\n",
        "# define the batch size for the test dataloader\n",
        "batch_size = 2\n",
        "\n",
        "# create the test dataset\n",
        "test_dataset = OxfordPetDataset(test_images_filenames, test_images_directory, test_masks_directory, transform=test_transform)\n",
        "\n",
        "# create the test dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True)\n",
        "\n",
        "# create an empty confusion matrix to store the predictions\n",
        "conf_matrix = torch.zeros((6, 6), dtype=torch.int64)\n",
        "\n",
        "# put the model on CPU\n",
        "model = model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# iterate over the test dataloader\n",
        "with torch.no_grad():\n",
        "    for images, targets in test_loader:\n",
        "        outputs = model(images.float())\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        # calculate the confusion matrix for the current batch\n",
        "        conf_matrix += confusion_matrix(targets.flatten(), preds.flatten(), labels=list(range(6)))\n",
        "\n",
        "# calculate the IoU for each class\n",
        "class_IoU = torch.zeros(6)\n",
        "for i in range(6):\n",
        "    class_IoU[i] = conf_matrix[i,i] / (conf_matrix[i,:].sum() + conf_matrix[:,i].sum() - conf_matrix[i,i])\n",
        "\n",
        "# calculate the mean IoU\n",
        "mIoU = class_IoU.mean()\n",
        "print(\"Mean IoU: {:.4f}\".format(mIoU))\n",
        "print(f'IoU of each class: {class_IoU}')\n",
        "\n",
        "# calculate the precision for each class\n",
        "class_precision = torch.zeros(6)\n",
        "for i in range(6):\n",
        "    class_precision[i] = conf_matrix[i,i] / conf_matrix[:,i].sum()\n",
        "\n",
        "# calculate the recall for each class\n",
        "class_recall = torch.zeros(6)\n",
        "for i in range(6):\n",
        "    class_recall[i] = conf_matrix[i,i] / conf_matrix[i,:].sum()\n",
        "\n",
        "# replace NaNs in the precision and recall tensors with 0s\n",
        "class_precision = torch.where(torch.isnan(class_precision), torch.tensor(0.0), class_precision)\n",
        "class_recall = torch.where(torch.isnan(class_recall), torch.tensor(0.0), class_recall)\n",
        "\n",
        "# print the precision, recall, and f1-score for each class\n",
        "print('class_precision', class_precision)\n",
        "print(\"Mean Precision: {:.4f}\".format(class_precision.mean()))\n",
        "# (0.7981 + 0.9423 + 0.7577 + 0.7194 + 0.4961 + 0) / 6\n",
        "\n",
        "print('class_recall', class_recall)\n",
        "print(\"Mean Recall: {:.4f}\".format(class_recall.mean()))\n",
        "\n",
        "# calculate the f1-score for each class\n",
        "class_f1_score = torch.zeros(6)\n",
        "for i in range(6):\n",
        "    class_f1_score[i] = 2 * (class_precision[i] * class_recall[i]) / (class_precision[i] + class_recall[i])\n",
        "\n",
        "# calculate the mean f1-score\n",
        "mean_f1_score = class_f1_score.mean()\n",
        "\n",
        "# replace NaNs in the f1-score tensor with 0s\n",
        "class_f1_score = torch.where(torch.isnan(class_f1_score), torch.tensor(0.0), class_f1_score)\n",
        "\n",
        "# print class-wise f1-score and mean f1-score\n",
        "print('class_f1_score', class_f1_score)\n",
        "print('class_f1_score.mean()', class_f1_score.mean())\n",
        "\n",
        "# calculate the accuracy\n",
        "accuracy = torch.diag(conf_matrix).sum() / conf_matrix.sum()\n",
        "\n",
        "# print mean accuracy\n",
        "print(f\"Mean Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtseCTjoldsv"
      },
      "source": [
        "### Confusion Matrix\n",
        "The confusion matrix is a table that shows the number of true positives, false positives, true negatives, and false negatives for each class in the classification problem.\n",
        "\n",
        "The `plt.imshow()` function displays the confusion matrix as a color-coded image, where each cell of the matrix is represented by a color intensity based on its value. In this case, the `cmap='Blues'` argument specifies that the colors used should range from light to dark blue. The `plt.colorbar()` function adds a color bar to the side of the image to indicate the values corresponding to each color. The `plt.xlabel()` and `plt.ylabel()` functions add labels to the x and y axes, respectively, indicating the predicted and true labels. Finally, `plt.show()` displays the image.\n",
        "\n",
        "By looking at the image generated by this code, we can quickly see which classes are being classified correctly and which ones are being misclassified. The diagonal line of the matrix represents the correctly classified instances, and the off-diagonal elements indicate the number of misclassifications. The darker the color of a cell, the higher the number of samples that were classified in that particular way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "ToqZD5Nx7yAt",
        "outputId": "82d4a4bf-dd2e-44aa-d91c-0a1c3605b000"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(conf_matrix, cmap='Blues')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iwetnw7w6CE"
      },
      "source": [
        "## Inference Visualization\n",
        "The function takes the following arguments:\n",
        "\n",
        "* `images_filenames`: A list of image filenames.\n",
        "* `images_directory`: A string representing the directory path where the images are located.\n",
        "* `masks_directory`: A string representing the directory path where the masks are located.\n",
        "* `predicted_masks`: A list of predicted masks (each predicted mask must have the same dimensions as its corresponding image).\n",
        "\n",
        "The function starts by defining some variables such as the number of images to be displayed (n), the class names, the number of classes, and the color map. Then, for each image in the list of images, the function loads the image and its corresponding ground truth mask from their respective directories.\n",
        "\n",
        "Next, the function plots the image in the first column of the grid, and the ground truth mask in the second column. The ground truth mask is overlaid on top of the image using a color map that maps each pixel value to its corresponding class color.\n",
        "\n",
        "If `predicted_masks` are provided, the function plots them in the third column of the grid in the same way as the ground truth mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vONh-V2JeeK"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "def display_image_grid(images_filenames, images_directory, masks_directory, predicted_masks=None):\n",
        "\n",
        "    # define the number of images to be displayed per row\n",
        "    n = 3\n",
        "\n",
        "    for i, image_filename in enumerate(images_filenames):\n",
        "        # define the size of the resulting plot\n",
        "        plt.figure(figsize=(20, 7))\n",
        "\n",
        "        font_size = 'medium'\n",
        "\n",
        "        # define the class names for the semantic segmentation task\n",
        "        class_names = [\n",
        "            'Background',\n",
        "            'Estratocumuliform',\n",
        "            'Estratiform',\n",
        "            'Cirriform',\n",
        "            'Cumuliform'\n",
        "        ]\n",
        "\n",
        "        # get the total number of classes\n",
        "        n_classes = len(class_names)\n",
        "\n",
        "        # get the colormap for the visualization\n",
        "        cmap = plt.cm.get_cmap('tab20c', n_classes)\n",
        "\n",
        "        # read the input image and the corresponding ground truth mask\n",
        "        image_filename = images_filenames[len(images_filenames)-(i+1)]\n",
        "        image = cv2.imread(os.path.join(images_directory, image_filename))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(os.path.join(masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,)\n",
        "        mask = preprocess_mask(mask)\n",
        "\n",
        "        # display the original image in the first column\n",
        "        plt.subplot(1, n, 1)\n",
        "        plt.title(\"Image\", fontsize=font_size)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(image)\n",
        "\n",
        "        # create a colorbar for the ground truth mask visualization\n",
        "        plt.clim(0, n_classes-1)\n",
        "        clb = plt.colorbar(shrink=0.55)\n",
        "        clb.set_ticks(range(n_classes))\n",
        "        clb.ax.tick_params(labelsize=10)\n",
        "        clb.ax.set_yticklabels(class_names)\n",
        "        clb.ax.set_visible(False)\n",
        "\n",
        "        # set the alpha value for the mask visualization\n",
        "        mask_alpha = .5\n",
        "\n",
        "        # display the ground truth mask in the second column\n",
        "        plt.subplot(1, n, 2)\n",
        "        plt.title(\"Ground Truth\", fontsize=font_size)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(image)\n",
        "        plt.imshow(mask, interpolation=\"none\", alpha=mask_alpha, cmap=cmap)\n",
        "\n",
        "        # create a colorbar for the ground truth mask visualization\n",
        "        plt.clim(0, n_classes-1)\n",
        "        clb = plt.colorbar(shrink=0.55)\n",
        "        clb.set_ticks(range(n_classes))\n",
        "        clb.ax.tick_params(labelsize=10)\n",
        "        clb.ax.set_yticklabels(class_names)\n",
        "        clb.ax.set_visible(False)\n",
        "\n",
        "        # if the predicted masks are provided, display them in the third column\n",
        "        if predicted_masks is not None:\n",
        "            predicted_mask = predicted_masks[len(images_filenames)-(i+1)]\n",
        "\n",
        "            plt.subplot(1, n, 3)\n",
        "            plt.title(\"Inference\", fontsize=font_size)\n",
        "            plt.axis(\"off\")\n",
        "            plt.imshow(image)\n",
        "            plt.imshow(predicted_mask, interpolation=\"none\", alpha=mask_alpha, cmap=cmap)\n",
        "\n",
        "            # create a colorbar for the predicted mask visualization\n",
        "            plt.clim(0, n_classes-1)\n",
        "            clb = plt.colorbar(shrink=0.60, spacing='uniform')\n",
        "            clb.set_ticks(range(n_classes))\n",
        "            clb.ax.tick_params(labelsize=10)\n",
        "            clb.ax.set_yticklabels(class_names)\n",
        "            clb.ax.set_visible(True)\n",
        "\n",
        "        # adjust the layout of the plot\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # save the resulting plot to a file\n",
        "        plt.savefig('path_to_save/results/' + image_filename)\n",
        "        # plt.show()\n",
        "        # break\n",
        "        plt.clf()\n",
        "        plt.cla()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf0TWHhLKaFY"
      },
      "outputs": [],
      "source": [
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(1280, 1280),\n",
        "        ToTensorV2(),\n",
        "     ]\n",
        ")\n",
        "\n",
        "# create the test dataset\n",
        "test_dataset = OxfordPetInferenceDataset(test_images_filenames, test_images_directory, test_masks_directory, transform=test_transform)\n",
        "\n",
        "# create the test dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True)\n",
        "\n",
        "# Load the EfficientUNet-B0 model with specified parameters and load its weights from a saved checkpoint\n",
        "model = get_efficientunet_b0(out_channels=5, concat_input=True, pretrained=True).to(\"cpu\")\n",
        "model.load_state_dict(torch.load(\"path_to_save/13.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# put the model on CPU\n",
        "model = model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# iterate over the test dataloader\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (images, targets, filenames) in enumerate(test_loader):\n",
        "        outputs = model(images.float())\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        # get original image sizes from the dataset\n",
        "        original_sizes = [(cv2.imread(os.path.join(test_images_directory, filename))).shape[:2][::-1] for filename in filenames]\n",
        "\n",
        "        # resize the predicted masks to the original size\n",
        "        preds_resized = [cv2.resize(pred.cpu().numpy(), size, interpolation=cv2.INTER_NEAREST) for pred, size in zip(preds, original_sizes)]\n",
        "        preds_resized = torch.tensor(preds_resized)\n",
        "\n",
        "        display_image_grid(filenames, test_images_directory, test_masks_directory, predicted_masks=preds_resized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yDLCjhhevuM"
      },
      "outputs": [],
      "source": [
        "!rm -rf clouds1500_no_tree"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "22f9b13baaea1acbe777ec66c6afcb217233662477c06351c776723bc94750d1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}