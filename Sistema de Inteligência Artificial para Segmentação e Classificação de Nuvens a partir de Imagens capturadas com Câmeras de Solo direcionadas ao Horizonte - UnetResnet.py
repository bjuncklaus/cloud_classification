# -*- coding: utf-8 -*-
"""r34_no_tree_aug.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OSiZUid1cBNiu2jrnkwsZYpwUs2Q3tzW

# Libs

## Check GPU and CUDA version
"""

!nvidia-smi
!nvcc --version

"""## Install required libraries

"""

!pip install fastai==2.3.0
!pip install lapixdl==0.7.15
!pip install -U albumentations
!pip install --upgrade opencv-contrib-python==4.5.5.62

"""## Import required libraries"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

import fnmatch
import random
import shutil
import numpy as np
import matplotlib.pyplot as plt

from google.colab import drive
from pathlib import Path
from fastai.vision.all import *
import albumentations as A
import cv2

from lapixdl.evaluation.visualize import show_segmentations
from lapixdl.evaluation.model import Result
from fastai.vision.models import resnet18, resnet34

"""## Mount Google Drive

"""

drive.mount('/content/drive', force_remount=True)

"""# Initial Setup

## Set paths
"""

# Get the dataset
!cp "path_to_save/clouds1500_no_tree.zip" /content/
!mkdir /content/clouds1500_no_tree
!unzip "clouds1500_no_tree.zip" -d /content/clouds1500_no_tree

used_model = resnet34
path = Path('/content/')
path_model = Path('path_to_save/no_tree_aug')
path_dataset = path/'clouds1500_no_tree'
path_lbl = path_dataset/'all_masks'
path_img = path_dataset
path_models = Path("/content/Output")
lbl_names = get_image_files(path_lbl)

"""## Get filenames and masks

"""

fnames = get_image_files(path_img, folders=['train', 'validation'])
get_mask = lambda x: path_lbl/f'{x.stem}.png'
mask = PILMask.create(get_mask(fnames[1]))
src_size = np.array(mask.shape)

"""## Save class labels to a file

"""

config_file_content = '''Background
Estratocumuliformes
Estratiformes
Cirriformes
Cumuliformes'''

codes_save_path = "/content/codes.txt"

with open(codes_save_path, "w") as f:
    f.write(config_file_content)

codes = np.loadtxt(codes_save_path, dtype=str)
codes[0] = 'background' # To show in the visualization

"""# Metrics, augmentations, data split and model definition

## Fix seed
"""

import imgaug
import random
random.seed(81615)
imgaug.seed(81615)

"""## Define augmentation classes and functions

### Define a custom transform class to apply Albumentations augmentations during the data processing pipeline
"""

class SegmentationAlbumentationsTransform(ItemTransform):
    split_idx = 0  # Apply this transform only during training
    order = 2      # Apply this transform after resizing

    def __init__(self, aug): self.aug = aug

    def encodes(self, x):
        img, mask = x
        aug = self.aug(image=np.array(img), mask=np.array(mask))
        return PILImage.create(aug["image"]), PILMask.create(aug["mask"])

# Define a custom transform class to resize images and masks
class ImageResizer(Transform):
    order = 1

    "Resize image to `size` using `resample`"
    def __init__(self, size, resample=Image.BILINEAR):
        if not is_listy(size): size = (size, size)
        self.size, self.resample = (size[1], size[0]), resample

    def encodes(self, o: PILImage): return o.resize(size=self.size, resample=self.resample)
    def encodes(self, o: PILMask): return o.resize(size=self.size, resample=Image.NEAREST)

# Define a lambda function to create a zoom augmentation pipeline
zoom_augmentation = lambda img_shape: A.Compose([
    A.RandomScale(scale_limit=(0, 0.1), p=0.75),
    A.CenterCrop(img_shape[0], img_shape[1])
])

# Define a lambda function to create an augmentation pipeline
# In this case, the pipeline is not used in the initial experiments
augmentations = lambda img_shape: A.Compose([
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5)
])

"""## Define transformations, dataset, and metrics

"""

# Define the transformations to be applied to the images and masks
tfms = [[PILImage.create], [get_mask, PILMask.create, AddMaskCodes(codes)]]

# Define a function splitter to split the dataset into training and validation sets
# based on the folder name of the input images
folder_split = FuncSplitter(lambda fname: Path(fname).parent.name == 'validation')

# Create a Datasets object with the input filenames, transformations, and the defined split function
src = Datasets(fnames, tfms, splits=folder_split(fnames))

"""## Averaged Dice metric (Macro F1) for multiclass target in segmentation"""

# Define a custom Dice metric class for multiclass target in segmentation
class DiceMulti(Metric):
    "Averaged Dice metric (Macro F1) for multiclass target in segmentation"

    # Initialize the Dice metric with the specified axis
    def __init__(self, axis=1): self.axis = axis

    # Reset the metric's internal state
    def reset(self): self.inter,self.union,self.total_area = {},{},{}

    # Accumulate the metric values over the predicted and target outputs
    def accumulate(self, learn):
        pred, targ = flatten_check(learn.pred.argmax(dim=self.axis), learn.y)
        for c in range(learn.pred.shape[self.axis]):
            p = torch.where(pred == c, 1, 0)
            t = torch.where(targ == c, 1, 0)
            c_inter = (p*t).float().sum().item()
            c_totalSumAreas = (p+t).float().sum().item()
            c_union = c_totalSumAreas-c_inter
            if c in self.inter:
                self.inter[c] += c_inter
                self.total_area[c] += c_totalSumAreas
                self.union[c] += c_union
            else:
                self.inter[c] = c_inter
                self.total_area[c] = c_totalSumAreas
                self.union[c] = c_union

    # Calculate the final Dice metric value
    @property
    def value(self):
        binary_dice_scores = np.array([])
        for c in self.inter:
            binary_dice_scores = np.append(binary_dice_scores, 2.*self.inter[c]/self.total_area[c] if self.total_area[c] > 0 else np.nan)
        return np.nanmean(binary_dice_scores)

"""## Implementation of the Averaged Jaccard coefficient that is lighter in RAM -- Mean IoU (Intersection Over Union)"""

class JaccardCoeffMulti(DiceMulti):
  @property
  def value(self):
    binary_jaccard_scores = np.array([])
    for c in self.inter:
        binary_jaccard_scores = np.append(binary_jaccard_scores, self.inter[c]/self.union[c] if self.union[c] > 0 else np.nan)
    return np.nanmean(binary_jaccard_scores)

"""## Define a function to calculate the accuracy metric for foreground segmentation
The `acc_metric` function calculates the foreground accuracy for segmentation tasks by calling the `foreground_acc` function and ignoring the background class with index 0 (`bkg_idx=0`).
"""

def acc_metric(ipt, target):
  return foreground_acc(ipt, target, bkg_idx=0)

# Create an instance of the Jaccard Coefficient metric (Mean IoU)
iou_metric = JaccardCoeffMulti()

# Create an instance of the Dice metric (Macro F1) for multiclass segmentation
f1_metric = DiceMulti()

# Combine the three metrics into a list
metrics = [acc_metric, iou_metric, f1_metric]

"""## Get a specified number of segmentation results by predicting on randomly selected images"""

def get_results(predict, size, n=3):
    # Loop through a random sample of 'n' file names from the 'fnames' list
    for fname in random.sample(fnames, n):
        # Get the prediction for the current file name
        res = predict(fname)

        # Yield a new Result instance with the following attributes:
        # 1. The resized image as a NumPy array
        # 2. The resized mask as a NumPy array
        # 3. The first element of the prediction result
        yield Result(
            np.array(PILImage.create(fname).resize((size[1], size[0]))),
            np.array(PILMask.create(get_mask(fname)).resize((size[1], size[0]))),
            np.array(res[0])
        )

"""## Get a segmentation result for a specific image"""

def get_result(predict, size, fname):
    # Get the prediction for the given file name
    res = predict(fname)

    # Yield a new Result instance with the following attributes:
    # 1. The resized image as a NumPy array
    # 2. The resized mask as a NumPy array
    # 3. The first element of the prediction result
    yield Result(
        np.array(PILImage.create(fname).resize((size[1], size[0]))),
        np.array(PILMask.create(get_mask(fname)).resize((size[1], size[0]))),
        np.array(res[0])
    )

"""# Training

## Learner for training the segmentation model
It takes the data, an optional model to load, and a flag to unfreeze the model
"""

def get_learner(data, load_model=None, unfreeze:bool=False):
    # Create a U-Net learner using the provided data, model, and metrics
    learn = unet_learner(data,
                         used_model,
                         metrics=metrics,
                         # loss_func=CrossEntropyLossFlat(axis=1) # default is already = CrossEntropyLossFlat
                         )

    # Set the learner's path to the 'path_models' directory
    learn.path = path_models

    # If a model is provided, load it with its optimizer state
    if(load_model != None):
        learn.load(load_model, with_opt=True)

    # If 'unfreeze' is set to True, unfreeze the learner's layers for fine-tuning
    if(unfreeze):
        learn.unfreeze()

    # Return the learner instance
    return learn

"""## Training Models

### 243 x 324
"""

# Set the size of the input images by dividing the source size by 8
size = src_size//8

# Set the batch size for training the model
bs = 60

# Define the list of image transformations to be applied to the input images
transforms = [
    # Resize the image to the specified size
    ImageResizer((size[0], size[1])),

    # Convert the image to a PyTorch tensor
    ToTensor(),

    # Convert the integer tensor to a float tensor
    IntToFloatTensor(),

    # Apply the Albumentations transformations for segmentation tasks
    SegmentationAlbumentationsTransform(augmentations(size))
]

# Create dataloaders for training and validation datasets using the source (src) and specified batch size (bs)
data = src.dataloaders(bs=bs, after_item=transforms)

# Display a batch of training images with their corresponding labels, limited to 6 images
data.train.show_batch(max_n=6)

"""#### The following code finds the optimal learning rate for training a U-Net model using FastAI's learning rate finder.

1. `get_learner(data)`: This function creates a U-Net learner using the provided data (image and mask pairs) from the data variable.

The learner is responsible for training the model and making predictions.

2. `.lr_find()`: This method is a part of the FastAI library and is used to find the optimal learning rate for training the model.

It trains the model for a few iterations while increasing the learning rate exponentially at each step. The method then plots the loss values against learning rates and helps to identify the optimal learning rate, usually the point where the loss starts to decrease rapidly before it starts increasing again.

##### How to use it:
1. First, you need to have created a dataset with images and their corresponding masks using FastAI's data block API.
2. Run the code, and it will plot the loss values against learning rates.
3. Observe the plot and find the optimal learning rate.

##### How to interpret it:
The learning rate finder plot displays the loss values on the y-axis and learning rates on the x-axis (in logarithmic scale). To interpret the plot, follow these steps:

1. Look for the steepest downward slope in the plot, which indicates the area where the loss is decreasing the fastest.
2. Pick a learning rate that is somewhere in the middle of this steepest slope. It's a good practice to choose a value slightly lower than the one where the minimum loss is obtained to ensure more stable training.
3. Use the chosen learning rate for training the model in further steps.

##### Example:
Let's assume you have created a dataset with images and their corresponding masks using FastAI's data block API. Then, you have run the code:

* `learner = get_learner(data)`
* `lr_min_plot = learner.lr_find()`

After observing the plot, you find that the optimal learning rate is 1e-3. You can then use this learning rate to train the model:

`learner.fit_one_cycle(10, lr_max=1e-3)`

In this example, the model will be trained for 10 epochs using the optimal learning rate of 1e-3.
"""

get_learner(data).lr_find()

# Set the learning rate to a value of 1e-3
lr = slice(9.120108734350652e-05)

# Set the weight decay to a value of 1e-3
wd = 1e-4

# Create a U-Net learner using the provided data
learn = get_learner(data)

# Create a SaveModelCallback, which will save the model with the best 'dice_multi' metric during training
# The saved model will be stored in the 'path_model' directory with the filename 'best_model_300x400_stg1' and its optimizer state
callback = SaveModelCallback(monitor='dice_multi', fname=path_model/'best_model_243x324_stg1', with_opt=True)

"""#### The following code trains the U-Net model using the 'fit_one_cycle' method with the specified number of epochs, learning rate, weight decay, and a callback.

1. `learn.fit_one_cycle()`: This method is a part of the FastAI library and is used to train the model using the 1cycle policy, which is an efficient and effective training method. The 1cycle policy adjusts the learning rate and momentum during training to help the model converge faster and achieve better performance.

2. `15`: The number of epochs to train the model. An epoch is a complete iteration through the training dataset.

3. `lr_max=lr`: The maximum learning rate to be used during training, which is set to the value of 'lr' (1e-3 in this case).

4. `wd=wd`: The weight decay to be used during training, which is set to the value of 'wd' (1e-3 in this case). Weight decay is a regularization technique that helps prevent overfitting by adding a penalty to the loss function based on the magnitude of the model's weights.

5. `cbs=callback`: The callback to be used during training. In this case, the `SaveModelCallback` is used, which saves the model with the best `dice_multi` metric during training. The saved model will be stored in the 'path_model' directory with the filename `best_model_300x400_stg1` and its optimizer state. Callbacks allow you to customize the training process by performing specific actions at various stages of training.

##### How to use it:
1. Make sure you have a learner (in this case, a U-Net learner) created using the `get_learner()` function with the data.
2. Set the desired number of epochs, learning rate, weight decay, and any callbacks you want to include.
3. Run the code to start the training process.

##### How to interpret it:
During the training process, FastAI will display a table showing the progress of the training. The table includes the following columns: epoch number, training loss, validation loss, and the values of any metrics specified when creating the learner (such as `dice_multi` in this case).

To interpret the training progress, monitor the following aspects:
1. Observe the validation loss and metric values to understand how well the model is performing on the validation dataset. Lower validation loss and higher metric values indicate better performance.
2. Check if the training loss is continuously decreasing, which means the model is learning from the training data.
3. Look for signs of overfitting or underfitting. Overfitting occurs when the training loss is significantly lower than the validation loss, indicating that the model is performing well on the training data but not generalizing well to new data. Underfitting occurs when the model doesn't perform well on either the training or validation data, which can be due to insufficient training or an inappropriate model architecture.

After the training is complete, you can further analyze the performance of the model by visualizing the predictions on the validation dataset and comparing them to the ground truth masks. If you're satisfied with the model's performance, you can use it for making predictions on new, unseen data. If the performance is not satisfactory, consider adjusting the training parameters, using a different model architecture, or adding more training data.
"""

learn.fit_one_cycle(15, lr_max=lr, wd=wd, cbs=callback)

"""#### The 'show_results()' method is used to display the model's predictions on a batch of validation data, along with the corresponding ground truth masks and input images.

`max_n=6`: This parameter limits the number of displayed image-mask-prediction sets to 6.

`figsize=(20,30)`: This parameter sets the size of the displayed figure to 20 inches wide and 30 inches tall.

To use this code, ensure that you have a trained `learn` object, which is the U-Net learner in this case. After training is complete, run the code to visualize the model's predictions on the validation dataset. By comparing the model's predictions to the ground truth masks, you can qualitatively assess the performance of the model and identify any areas where it may be struggling or performing well.
"""

learn.show_results(max_n=6, figsize=(15,25))

"""#### The following code performs learning rate finding for a U-Net model that has been fine-tuned using the 1cycle policy. The 'get_learner()' function is used to create a learner with the fine-tuned model, and the 'lr_find()' method is called to find the optimal learning rate.

1. `get_learner(data, path_model/'best_model_300x400_stg1', True)`: This function creates a U-Net learner using the provided data (image and mask pairs) from the 'data' variable. It loads the model weights from the file 'best_model_300x400_stg1' located in the 'path_model' directory. The 'True' parameter indicates that the learner's layers should be unfrozen for fine-tuning.

2. `.lr_find()`: This method is a part of the FastAI library and is used to find the optimal learning rate for training the model. It trains the model for a few iterations while increasing the learning rate exponentially at each step. The method then plots the loss values against learning rates and helps to identify the optimal learning rate, usually the point where the loss starts to decrease rapidly before it starts increasing again.

To use this code, make sure you have a dataset with images and their corresponding masks created using FastAI's data block API. Also, ensure that you have a fine-tuned model saved at 'path_model/'best_model_300x400_stg1'. Run the code, and it will plot the loss values against learning rates. Observe the plot and find the optimal learning rate for further fine-tuning of the model.
"""

get_learner(data, path_model/'best_model_243x324_stg1', True).lr_find()

# Set the learning rate range to values between 1e-5 and 1e-4
lr = slice(5.248074739938602e-05)

# Set the weight decay to a value of 1e-3
wd=1e-4

# Create a U-Net learner using the provided data, load the fine-tuned model from 'best_model_300x400_stg1', and unfreeze its layers
learn = get_learner(data, path_model/'best_model_243x324_stg1', True)

# Create a SaveModelCallback, which saves the model with the best 'dice_multi' metric during training
# The saved model will be stored in the 'path_model' directory with the filename 'best_model_300x400_stg2' and its optimizer state
callback = SaveModelCallback(monitor='dice_multi', fname=path_model/'best_model_243x324_stg2', with_opt=True)

# Train the model for 30 epochs using the specified learning rate range, weight decay, and callback
learn.fit_one_cycle(30, lr_max=lr, wd=wd, cbs=callback)

# Create a U-Net learner using the provided data and load the model weights from the file 'best_model_300x400_stg2' located in the 'path_model' directory
learn = get_learner(data, path_model/'best_model_243x324_stg2')

"""#### The following code displays the segmentation results of the U-Net model on a random sample of images using the 'show_segmentations()' function.

1. get_results(learn.predict, size): This function generates a list of Result instances containing the resized input images, ground truth masks, and predicted masks for a random sample of images. The 'learn.predict' function is used to generate predictions from the U-Net model.

2. list(): Convert the generator returned by 'get_results()' into a list.

3. show_segmentations(): This function displays the input images, ground truth masks, and predicted masks side by side, with the masks overlayed on the images. The 'codes' parameter specifies the class labels for the segmentation, and 'mask_alpha=0.5' sets the transparency of the masks to 50%.

After running the code, you will see a visualization of the model's predictions on a random sample of images, allowing you to qualitatively assess the performance of the model.
"""

fig, axes = show_segmentations(list(get_results(learn.predict, size)), codes, mask_alpha=0.5)

"""### 486 x 648"""

# Set the size of input images by dividing the source size by 4
size = src_size//4

# Set the batch size for training the model
bs = 16

# Define the list of image transformations to be applied to the input images
transforms = [
    ImageResizer((size[0], size[1])),
    ToTensor(),
    IntToFloatTensor(),
    SegmentationAlbumentationsTransform(augmentations(size))
]

# Create dataloaders for training and validation datasets using the source (src) and specified batch size (bs)
data = src.dataloaders(bs=bs, after_item=transforms)

"""#### The following code performs learning rate finding for a U-Net model that has been previously trained using a lower input resolution.

* The `get_learner()` function is used to create a learner with the new higher-resolution
data and the previously trained model, and the `lr_find()` method is called to find the optimal learning rate for further fine-tuning.

* The reason for increasing the input resolution now is to improve the model's ability to capture finer details in the images, which can lead to better segmentation performance. Training the model initially with a lower resolution helps to capture coarse, global features more efficiently, while fine-tuning at a higher resolution focuses on learning more fine-grained, local features. This approach is known as progressive resizing and is a common technique for improving the performance of deep learning models while reducing training time.
"""

get_learner(data, path_model/'best_model_243x324_stg2').lr_find()

# Set the learning rate range to values between 1e-4 and 1e-3
lr=slice(1.2022644114040304e-05)

# Set the weight decay to a value of 1e-3
wd=1e-4

# Create a U-Net learner using the provided higher-resolution data
learn = get_learner(data)

# Create a SaveModelCallback, which saves the model with the best 'dice_multi' metric during training
# The saved model will be stored in the 'path_model' directory with the filename 'best_model_600x800_stg1' and its optimizer state
callback = SaveModelCallback(monitor='dice_multi', fname=path_model/'best_model_486x648_stg1', with_opt=True)

# Train the model for 15 epochs using the specified learning rate range, weight decay, and callback
learn.fit_one_cycle(15, lr_max=lr, wd=wd, cbs=callback)

get_learner(data, path_model/'best_model_486x648_stg1', True).lr_find()

lr = slice(1.4454397387453355e-05)
wd=1e-4
learn = get_learner(data, path_model/'best_model_486x648_stg1', True)
callback = SaveModelCallback(monitor='dice_multi', fname=path_model/'best_model_486x648_stg2', with_opt=True)

learn.fit_one_cycle(30, lr_max=lr, wd=wd, cbs=callback)

learn = get_learner(data, path_model/'best_model_486x648_stg2')

fig, axes = show_segmentations(list(get_results(learn.predict, size)), codes, mask_alpha=0.5)

"""### 972 x 1296

#### Now we follow the same principle as before, increasing the resolution a bit more and training it again.
"""

size = src_size//2
bs = 4
transforms = [
  ImageResizer((size[0], size[1])),
  ToTensor(),
  IntToFloatTensor(),
  SegmentationAlbumentationsTransform(augmentations(size))
]
print(size)

data = src.dataloaders(bs=bs, after_item=transforms)

get_learner(data, path_model/'best_model_486x648_stg2').lr_find()

lr=slice(2.75422871709452e-06)
wd=1e-4
learn = get_learner(data, path_model/'best_model_486x648_stg2')
callback = SaveModelCallback(monitor='dice_multi', fname=path_model/'best_model_972x1296_stg1', with_opt=True)

learn.fit_one_cycle(15, lr_max=lr, wd=wd, cbs=callback)

get_learner(data, path_model/'best_model_972x1296_stg1', True).lr_find()

lr = slice(9.120108734350652e-05)
wd=1e-4
learn = get_learner(data, path_model/'best_model_972x1296_stg1', True)
callback = SaveModelCallback(monitor='dice_multi', fname=path_model/'best_model_972x1296_stg2', with_opt=True)

learn.fit_one_cycle(40, lr_max=lr, wd=wd, cbs=callback)

fig, axes = show_segmentations(list(get_results(learn.predict, size)), codes, mask_alpha=0.5)

"""# My plotting function to show inference results"""

def create_directory(directory_path):
    if not os.path.exists(directory_path):
        os.makedirs(directory_path)

from matplotlib import cm
from matplotlib.colors import ListedColormap
import matplotlib.patches as mpatches

def my_show_segmentations(results, class_names, subdir='', fname=None):

    n = 3

    for i, result in enumerate(results):
        plt.figure(figsize=(20, 7))

        # cmap = 'jet'
        # cmap = 'tab20c'

        # cmap_colors = cm.get_cmap(cmap).colors
        font_size = 'medium'

        class_names = [
            'Background',
            'Estratocumuliformes',
            'Estratiformes',
            'Cirriformes',
            'Cumuliformes'
        ]

        cmap_colors = [
            (31/255, 119/255, 180/255),
            (43/255, 160/255, 43/255),
            (214/255, 39/255, 39/255),
            (148/255, 103/255, 189/255),
            (140/255, 85/255, 76/255),
        ]

        # create a colormap from these colors
        cmap = ListedColormap(cmap_colors)

        n_classes = len(class_names)

        if fname:
            image_filename = fname
        else:
            image_filename = fnames[i]
        image = result.image

        mask = result.gt

        plt.subplot(1, n, 1)
        plt.title("Image", fontsize=font_size)
        plt.axis("off")
        plt.imshow(image)

        plt.clim(0, n_classes-1)
        clb = plt.colorbar(shrink=0.55)
        clb.set_ticks(range(n_classes))
        clb.ax.tick_params(labelsize=10)
        clb.ax.set_yticklabels(class_names)
        clb.ax.set_visible(False)

        mask_alpha = .5

        plt.subplot(1, n, 2)
        plt.title("Ground truth mask", fontsize=font_size)
        plt.axis("off")
        plt.imshow(image)
        plt.imshow(mask, interpolation="none", alpha=mask_alpha, cmap=cmap)

        plt.clim(0, n_classes-1)
        clb = plt.colorbar(shrink=0.55)
        clb.set_ticks(range(n_classes))
        clb.ax.tick_params(labelsize=10)
        clb.ax.set_yticklabels(class_names)
        clb.ax.set_visible(False)

        predicted_mask = result.prediction

        plt.subplot(1, n, 3)
        plt.title("Predicted mask", fontsize=font_size)
        plt.axis("off")
        plt.imshow(image)
        plt.imshow(predicted_mask, interpolation="none", alpha=mask_alpha, cmap=cmap)

        plt.clim(0, n_classes-1)
        clb = plt.colorbar(shrink=0.60, spacing='uniform')
        clb.set_ticks(range(n_classes))
        clb.ax.tick_params(labelsize=10)
        clb.ax.set_yticklabels(class_names)
        clb.ax.set_visible(True)


        directory_path = str(path_model) + f'/results/{subdir}/inference/'
        create_directory(directory_path)

        plt.tight_layout()
        plt.savefig(directory_path + str(image_filename).split('/')[-1])
        plt.clf()
        plt.cla()

"""# Evaluation

This code defines two functions, `gt_mask_iterator_from_image_files()` and `pred_mask_iterator_from_image_files()`, which are used to create iterators for ground truth (GT) masks and predicted masks, respectively, when given a list of image file names and a desired output size.

1. `from lapixdl.evaluation.evaluate import evaluate_segmentation`: This line imports the `evaluate_segmentation` function from the `lapixdl.evaluation.evaluate` module. This function can be used to evaluate the performance of a segmentation model by comparing its predictions to the ground truth masks.

2. `gt_mask_iterator_from_image_files(fnames, size)`: This function takes a list of image file names (`fnames`) and a desired output size (`size`) as input arguments. It iterates through the file names, loads the corresponding ground truth masks, resizes them to the specified size, and yields the resized masks one by one. The function is a generator, which means it can be used to create an iterator, allowing efficient processing of large datasets.

3. `pred_mask_iterator_from_image_files(fnames, size, predict)`: This function takes a list of image file names (`fnames`), a desired output size (`size`), and a prediction function (`predict`) as input arguments. It iterates through the file names, generates predictions for each image using the `predict` function, and yields the predicted masks one by one. Like the previous function, this function is also a generator, which can be used to create an iterator for efficient processing.

These two functions can be used in combination with the `evaluate_segmentation` function to evaluate the performance of a segmentation model on a dataset. By providing iterators for ground truth masks and predicted masks, the evaluation can be performed efficiently, even on large datasets.
"""

from lapixdl.evaluation.evaluate import evaluate_segmentation

def gt_mask_iterator_from_image_files(fnames, size):
  for fname in fnames:
    yield np.array(PILMask.create(get_mask(fname)).resize((size[1], size[0])))

def pred_mask_iterator_from_image_files(fnames, size, predict):
  for fname in fnames:
    res = predict(fname)
    yield np.array(res[0])

"""This code prepares the test dataset for evaluation by creating data loaders for the test images and applying the necessary transforms to the images.

1. `size = (Y, X)`: Set the desired size of the input images for the test dataset. In this case, the height is Y and the width is X.

2. `test_image_files = get_image_files(path_img/'test_images')`: Get the list of image file names from the 'test_images' directory located in 'path_img'.

3. `transforms = [ImageResizer((size[0], size[1])), ToTensor(), IntToFloatTensor()]`: Define the list of image transformations to be applied to the input test images. These include resizing the images to the specified size, converting them to PyTorch tensors, and converting the integer tensors to float tensors.

4. `tfms = [[PILImage.create], [get_mask, PILMask.create, AddMaskCodes(codes)]]`: Define the list of item transforms for the test dataset. For the input images, create a PILImage, and for the masks, apply the 'get_mask' function to fetch the corresponding mask file name, create a PILMask, and add the mask codes using the 'AddMaskCodes' function.

5. `src = Datasets(test_image_files, tfms)`: Create a FastAI 'Datasets' object using the test image file names and the defined item transforms.

6. `test_dl = src.dataloaders(bs=1, after_item=transforms)`: Create data loaders for the test dataset using the 'Datasets' object, applying the defined image transformations (resizing, tensor conversion, etc.) after loading the items. Set the batch size to 1 for evaluation.

This code sets up the test dataset and data loader, which can be used to evaluate the performance of the segmentation model on the test images.
"""

size = src_size//2
bs = 4
transforms = [
  ImageResizer((size[0], size[1])),
  ToTensor(),
  IntToFloatTensor(),
  SegmentationAlbumentationsTransform(augmentations(size))
]

data = src.dataloaders(bs=bs, after_item=transforms)
learn = get_learner(data, path_model/'best_model_972x1296_stg2')

size = (972, 1296)

test_image_files = get_image_files(path_img/'test')
transforms = [
  ImageResizer((size[0], size[1])),
  ToTensor(),
  IntToFloatTensor()
]

tfms = [[PILImage.create], [get_mask, PILMask.create, AddMaskCodes(codes)]]
src = Datasets(test_image_files, tfms)
test_dl = src.dataloaders(bs=1, after_item=transforms)

for fname in test_image_files:
    my_show_segmentations(list(get_result(learn.predict, size, fname)), codes, str(size).replace(', ', 'x').replace('(', '').replace(')', ''), fname=fname)

"""This code creates two iterators for ground truth masks and predicted masks using the previously defined `gt_mask_iterator_from_image_files()` and `pred_mask_iterator_from_image_files()` functions and the test image files.

1. `gt_masks = gt_mask_iterator_from_image_files(test_image_files, size)`: Create an iterator for ground truth masks using the list of test image files and the specified size. The iterator will yield the resized ground truth masks one by one for each image in the test dataset.

2. `pred_masks = pred_mask_iterator_from_image_files(test_image_files, size, learn.predict)`: Create an iterator for predicted masks using the list of test image files, the specified size, and the `learn.predict` function of the trained U-Net model. The iterator will yield the predicted masks one by one for each image in the test dataset.

These iterators can be used to efficiently evaluate the performance of the segmentation model on the test dataset by comparing the predicted masks to the ground truth masks.
"""

gt_masks = gt_mask_iterator_from_image_files(test_image_files, size)
pred_masks = pred_mask_iterator_from_image_files(test_image_files, size, learn.predict)

"""This code evaluates the performance of the segmentation model by comparing the predicted masks to the ground truth masks and displays the confusion matrix.

1. `eval = evaluate_segmentation(gt_masks, pred_masks, codes)`: Call the `evaluate_segmentation()` function using the ground truth masks iterator (`gt_masks`), predicted masks iterator (`pred_masks`), and the class labels (`codes`). This function computes the evaluation metrics, such as precision, recall, F1-score, and accuracy, for each class label in the segmentation task.

2. `eval.show_confusion_matrix()`: Call the `show_confusion_matrix()` method of the `eval` object to display the confusion matrix. The confusion matrix is a table that shows the number of true positives, false positives, true negatives, and false negatives for each class label. It helps to visualize the performance of the segmentation model and identify the areas where the model might be performing well or struggling.

By analyzing the confusion matrix, you can gain insights into the model's performance and identify opportunities for improvement, such as adjusting the training parameters, using a different model architecture, or adding more training data.
"""

eval = evaluate_segmentation(gt_masks, pred_masks, codes)
eval.show_confusion_matrix()

size = src_size//4
bs = 16
transforms = [
    ImageResizer((size[0], size[1])),
    ToTensor(),
    IntToFloatTensor(),
    SegmentationAlbumentationsTransform(augmentations(size))
]
data = src.dataloaders(bs=bs, after_item=transforms)
learn = get_learner(data, path_model/'best_model_486x648_stg2')

size = (486, 648)

test_image_files = get_image_files(path_img/'test')
transforms = [
  ImageResizer((size[0], size[1])),
  ToTensor(),
  IntToFloatTensor()
]

tfms = [[PILImage.create], [get_mask, PILMask.create, AddMaskCodes(codes)]]
src = Datasets(test_image_files, tfms)
test_dl = src.dataloaders(bs=1, after_item=transforms)

for fname in test_image_files:
    my_show_segmentations(list(get_result(learn.predict, size, fname)), codes, str(size).replace(', ', 'x').replace('(', '').replace(')', ''), fname=fname)

gt_masks = gt_mask_iterator_from_image_files(test_image_files, size)
pred_masks = pred_mask_iterator_from_image_files(test_image_files, size, learn.predict)
eval = evaluate_segmentation(gt_masks, pred_masks, codes)
eval.show_confusion_matrix()

size = src_size//8
bs = 60
transforms = [
    ImageResizer((size[0], size[1])),
    ToTensor(),
    IntToFloatTensor(),
    SegmentationAlbumentationsTransform(augmentations(size))
]
data = src.dataloaders(bs=bs, after_item=transforms)
learn = get_learner(data, path_model/'best_model_243x324_stg2')

size = (243, 324)

test_image_files = get_image_files(path_img/'test')
transforms = [
  ImageResizer((size[0], size[1])),
  ToTensor(),
  IntToFloatTensor()
]

tfms = [[PILImage.create], [get_mask, PILMask.create, AddMaskCodes(codes)]]
src = Datasets(test_image_files, tfms)
test_dl = src.dataloaders(bs=1, after_item=transforms)

for fname in test_image_files:
    my_show_segmentations(list(get_result(learn.predict, size, fname)), codes, str(size).replace(', ', 'x').replace('(', '').replace(')', ''), fname=fname)

gt_masks = gt_mask_iterator_from_image_files(test_image_files, size)
pred_masks = pred_mask_iterator_from_image_files(test_image_files, size, learn.predict)
eval = evaluate_segmentation(gt_masks, pred_masks, codes)
eval.show_confusion_matrix()